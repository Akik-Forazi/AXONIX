# axonix â€” Fully Local Super Agentic AI

```
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—
  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•
  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•     â–ˆâ–ˆâ•‘
  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘
  â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•â•  â•šâ•â•â•â•  â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â•   â•šâ•â•
```

**llama.cpp powered Â· Fully local Â· Zero cloud Â· Zero API keys**

---

## ğŸš€ Setup in 3 Steps

### Step 1 â€” Get llama.cpp

Download a prebuilt binary from [llama.cpp releases](https://github.com/ggerganov/llama.cpp/releases) (pick `llama-...-win-x64.zip` on Windows).

Download a GGUF model (e.g. from HuggingFace):
- [Llama 3 8B GGUF](https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF)
- [Mistral 7B GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)
- [DeepSeek Coder GGUF](https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF)

### Step 2 â€” Start llama.cpp server

```bash
# Windows
llama-server.exe -m your_model.gguf --port 8080 --ctx-size 4096 -ngl 99

# Linux / Mac
./llama-server -m your_model.gguf --port 8080 --ctx-size 4096 -ngl 99
```

The server exposes `http://localhost:8080` with OpenAI-compatible `/v1/chat/completions`.

### Step 3 â€” Install axonix

```bash
cd C:\Users\akikf\programing\nn
python -m pip install -e .
```

---

## ğŸ® Run Commands

| Command | Description |
|---------|-------------|
| `axonix run --lc` | Interactive local CLI loop (agent mode) |
| `axonix run --lc --w` | Interactive CLI **+ web UI** at localhost:7860 |
| `axonix run --lc -w --port 8888` | Custom web port |
| `axonix run --cli` | One-shot stdin mode |
| `axonix run agent "your task"` | Run agent on a specific task |
| `axonix web` | Start web UI only |

---

## ğŸŒ Web UI (`--w` flag)

```bash
axonix run --lc --w
```

Opens a beautiful dark-themed chat interface at **http://localhost:7860** with:

- **Chat mode** â€” direct conversation with the LLM
- **Agent mode** â€” full agentic loop with tool use, collapsible tool call explorer
- **Memory sidebar** â€” see what the agent has remembered
- **Live server status** â€” shows if llama.cpp is connected
- **Streaming support** â€” token-by-token output
- **Zero dependencies** â€” pure Python stdlib HTTP server, no Flask needed

---

## ğŸ› ï¸ Tools Available to the Agent

| Tool | Description |
|------|-------------|
| `file_read` | Read any file with line numbers |
| `file_write` | Create or overwrite files |
| `file_edit` | Find-and-replace edits |
| `file_delete` | Delete files or folders |
| `file_list` | List directory contents |
| `file_search` | Glob pattern file search |
| `file_append` | Append to a file |
| `shell_run` | Run any terminal command |
| `shell_python` | Execute Python code live |
| `web_get` | Fetch any URL |
| `web_search` | Search the web (DuckDuckGo, no API key) |
| `code_lint` | Lint Python with flake8 |
| `code_format` | Format Python with black |
| `code_tree` | Show project file tree |
| `memory_save` | Persist data between steps |
| `memory_get` | Recall persisted data |
| `memory_list` | List all memory keys |

---

## âš™ï¸ Configuration

```bash
axonix config show
axonix config set --url http://localhost:8080
axonix config set --model llama3-8b
axonix config set --steps 50 --temp 0.5 --tokens 4096
axonix config reset
```

Config is saved to `~/.axonix_config.json`.

### Per-run overrides

```bash
axonix run --lc --w --url http://localhost:8080 --steps 40 --temp 0.3
axonix run agent "fix main.py" --workspace ./myproject --tokens 4096
```

---

## ğŸ Python API

```python
from axonix import Agent

agent = Agent(
    base_url="http://localhost:8080",   # your llama.cpp server
    model="local",
    workspace="./myproject",
    max_steps=25,
    temperature=0.7,
)

# Full agentic run
result = agent.run("Create a FastAPI app with 3 endpoints and write it to main.py")

# Simple chat
response = agent.chat("What is a GGUF file?")

# Streaming chat
for token in agent.chat_stream("Explain llama.cpp in simple terms"):
    print(token, end="", flush=True)
```

---

## ğŸ“ Project Structure

```
nn/
â”œâ”€â”€ axonix/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ agent.py          # Agent loop + tool calling
â”‚   â”‚   â”œâ”€â”€ runner.py         # CLI dispatcher (all run modes)
â”‚   â”‚   â”œâ”€â”€ llama_backend.py  # llama.cpp HTTP client
â”‚   â”‚   â”œâ”€â”€ memory.py         # Persistent memory store
â”‚   â”‚   â””â”€â”€ config.py         # Config management
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â”œâ”€â”€ file_tools.py     # File operations
â”‚   â”‚   â”œâ”€â”€ shell_tools.py    # Shell + Python execution
â”‚   â”‚   â”œâ”€â”€ web_tools.py      # Fetch + DuckDuckGo search
â”‚   â”‚   â””â”€â”€ code_tools.py     # Lint / format / tree
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â””â”€â”€ specialized.py    # CoderAgent, ResearchAgent, FileAgent
â”‚   â””â”€â”€ web/
â”‚       â”œâ”€â”€ server.py          # Pure-stdlib HTTP server + REST API
â”‚       â””â”€â”€ static/
â”‚           â””â”€â”€ index.html     # Full chat web UI
â”œâ”€â”€ setup.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ’¡ Example Tasks

```bash
axonix run agent "Read all Python files and summarize what this project does"
axonix run agent "Create a complete Flask REST API with SQLite database"
axonix run agent "Search the web for best GGUF models under 8B parameters"
axonix run agent "Find all TODO comments in the codebase and create a todo.md"
axonix run agent "Write unit tests for every function in utils.py"
axonix run agent "Refactor this code to use dataclasses"
```

---

## CLI Shortcuts (in `--lc` mode)

| Input | Action |
|-------|--------|
| `agent` | Switch to agent mode |
| `chat` | Switch to direct chat mode |
| `reset` | Clear conversation history |
| `memory` | Show memory contents |
| `health` | Check llama.cpp server status |
| `!command` | Run shell command directly |
| `exit` / `quit` | Exit |

---

## Requirements

- Python **3.9+**
- **llama.cpp server** running locally
- **Zero pip dependencies** (stdlib only)
- Optional: `flake8`, `black` for code tools
